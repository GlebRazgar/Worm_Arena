# Model Configuration

# Device priority: MPS (Apple Silicon) > CUDA > CPU
device: "auto"  # Will auto-detect: mps > cuda > cpu

# =============================================================================
# Simple GCN Test Model (for quick testing)
# =============================================================================
simple_gcn:
  hidden_dim: 32
  epochs: 100
  batch_size: 32
  lr: 0.001

# =============================================================================
# GAT-LSTM Model (Main Model) - ULTRA OPTIMIZED FOR 36GB RAM
# =============================================================================
gat_lstm:
  # GAT Architecture (minimal for speed)
  gat:
    in_channels: 1                  # Input feature dimension (calcium activity)
    hidden_channels: [32]           # SINGLE layer for speed
    heads: [2]                      # Reduced heads
    edge_dim: 2                     # Edge feature dimension (gap + chem)
    dropout: 0.2                    # No dropout for speed
    add_self_loops: true            # Add self-loops to graph
    negative_slope: 0.2             # LeakyReLU negative slope
  
  # LSTM Architecture (minimal)
  lstm:
    hidden_size: 32                 # Reduced for memory
    num_layers: 1                   # Single layer
    dropout: 0.2                    # No dropout
    bidirectional: false            # Unidirectional
  
  # Prediction Head
  predictor:
    hidden_dims: [16]               # Minimal
    dropout: 0.2                    # No dropout
  
  # Residual Connection
  residual: true                    # IMPORTANT: keeps baseline performance
  
  # Optimization flags
  use_vectorized: true              # Use vectorized forward pass

# =============================================================================
# Training Configuration - ULTRA OPTIMIZED FOR 36GB RAM
# =============================================================================
training:
  # Data (MINIMAL for 36GB unified memory)
  window_size: 5                    # Minimal window
  stride: 5                         # Same as window (no overlap)
  batch_size: 32                    # Smaller batch to avoid swap
  max_worms: 5                      # 5 worms = ~33K samples, fits in RAM
  
  # Optimization
  lr: 0.001                         # Learning rate
  weight_decay: 0.0001              # L2 regularization
  epochs: 100                       # Maximum epochs
  patience: 15                      # Early stopping patience
  
  # Checkpointing
  save_every: 10                    # Save checkpoint every N epochs
  checkpoint_dir: "models/checkpoints"
  weights_dir: "models/weights"
  
  # Logging
  log_every: 1                      # Log every N epochs
  use_wandb: false                  # Use Weights & Biases logging

# =============================================================================
# A100 Configuration (default for new train.py)
# =============================================================================
gat_lstm_a100:
  gat:
    in_channels: 1
    hidden_channels: [128, 128]     # Deeper spatial model for CUDA
    heads: [4, 4]
    edge_dim: 2
    dropout: 0.2
    add_self_loops: true
    negative_slope: 0.2

  lstm:
    hidden_size: 128
    num_layers: 2
    dropout: 0.2
    bidirectional: false

  predictor:
    hidden_dims: [64, 32]
    dropout: 0.2

  residual: true
  use_vectorized: true

training_a100:
  # Data
  window_size: 10
  batch_size: 256
  max_worms: null                   # Use all worms
  num_workers: 4
  target_horizon: 10                # Default rollout horizon (Option D)

  # Optimization
  lr: 0.003
  weight_decay: 0.0001
  epochs: 150
  patience: 25

  # Logging/checkpointing
  save_every: 10
  checkpoint_dir: "models/checkpoints"
  weights_dir: "models/weights"
  log_every: 1
  use_wandb: true

# =============================================================================
# Evaluation
# =============================================================================
evaluation:
  horizons: [1, 10, 20]             # Prediction horizons to evaluate
  metrics: ["mse", "mae", "rmse", "r2", "correlation"]

# Full GNN-LSTM Model (for future use)
# model_type: gnn
# hidden_dim: 128
# layers: 4
# dropout: 0.1
# activation: relu

# # GNN-specific
# gnn:
#   conv: gat
#   heads: 4
#   readout: sum